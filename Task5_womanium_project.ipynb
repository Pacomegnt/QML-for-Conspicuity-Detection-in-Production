{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 5**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bimjipLPSngW"
   },
   "source": [
    "#### **Step 0:** Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "f9FcsEeBSnAv"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import json\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pennylane as qml\n",
    "import matplotlib.pyplot as plt\n",
    "from pennylane import numpy as pnp\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_curve, auc\n",
    "from contextlib import contextmanager\n",
    "from tabulate import tabulate\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xb5KZ8v8bdww"
   },
   "source": [
    "#### **Step 1:** Load and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vaGfo2LEkPPO"
   },
   "outputs": [],
   "source": [
    "# Count the total number of images in the folder\n",
    "def count_subfolders_and_files(path_dir):\n",
    "    total_files = sum(len(files) for _, _, files in os.walk(path_dir))\n",
    "    print(f\"Total number of images: {total_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M3x3urkiln7M",
    "outputId": "b67d34d2-a9ea-478b-c620-895ee2af3fd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images: 1801\n"
     ]
    }
   ],
   "source": [
    "count_subfolders_and_files('./dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_data(path_dir, json_file, img_size=28, max_images_per_class=None):\n",
    "    json_path = os.path.join(path_dir, json_file)\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # Load JSON data\n",
    "    with open(json_path, 'r') as file:\n",
    "        annotations = json.load(file)\n",
    "        \n",
    "    # Organize images by class\n",
    "    class_images = {}\n",
    "    for img_path, label in annotations.items():\n",
    "        full_img_path = os.path.join(path_dir, img_path)\n",
    "        if label not in class_images:\n",
    "            class_images[label] = set()\n",
    "        class_images[label].add(full_img_path)\n",
    "    \n",
    "    # Print total number of images found for each class\n",
    "    for label, paths in class_images.items():\n",
    "        print(f\"Total number of images found for class {label}: {len(paths)}\")\n",
    "    print()\n",
    "    \n",
    "    # Limit to max_images_per_class randomly and resize images\n",
    "    image_count_per_class = {}\n",
    "    for label, paths in class_images.items():\n",
    "        # Convert set to list for sampling\n",
    "        paths_list = list(paths)\n",
    "        selected_images = random.sample(paths_list, min(len(paths_list), max_images_per_class))\n",
    "        count = 0\n",
    "        skipped = 0\n",
    "        for img_path in selected_images:\n",
    "            image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if image is not None:\n",
    "                # Resize the image\n",
    "                image = cv2.resize(image, (img_size, img_size))\n",
    "                # Add channel dimension\n",
    "                image = image[:, :, np.newaxis]\n",
    "                images.append(image)\n",
    "                labels.append(label)\n",
    "                count += 1\n",
    "            else:\n",
    "                print(f\"Failed to load image: {img_path}\")\n",
    "                skipped += 1\n",
    "        image_count_per_class[label] = count\n",
    "        print(f\"Class {label}: Copied {count}, Skipped {skipped}\")\n",
    "        \n",
    "    # Convert list of images to numpy array and normalize to [0, 1]\n",
    "    images = np.array(images, dtype='float32') / 255.0\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    print()\n",
    "    # Print total images and images copied per class\n",
    "    tot_imgs = sum(image_count_per_class.values())\n",
    "    counts = \", \".join(f\"{k}:{v}\" for k, v in image_count_per_class.items())\n",
    "    print(f\"Total images: {tot_imgs}, Images copied per class: {counts}\")\n",
    "    \n",
    "    # Split into train, test, and validation sets\n",
    "    train_imgs, test_imgs_temp, train_labels, test_labels_temp = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "    test_imgs, val_imgs, test_labels, val_labels = train_test_split(test_imgs_temp, test_labels_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    print()\n",
    "    print(\"Number of images after splitting:\\n\")\n",
    "    print(f\"Training images: {len(train_imgs)}, Testing images: {len(test_imgs)}, Validation images: {len(val_imgs)}\")\n",
    "    \n",
    "    return (train_imgs, train_labels), (test_imgs, test_labels), (val_imgs, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AbkIt5RGp6da",
    "outputId": "5faf34af-0c55-426a-ce76-01333cb1d14f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images found for class 0: 300\n",
      "Total number of images found for class 1: 300\n",
      "Total number of images found for class 2: 300\n",
      "Total number of images found for class 3: 300\n",
      "Total number of images found for class 4: 300\n",
      "Total number of images found for class 5: 300\n",
      "\n",
      "Class 0: Copied 200, Skipped 0\n",
      "Class 1: Copied 200, Skipped 0\n",
      "Class 2: Copied 200, Skipped 0\n",
      "Class 3: Copied 200, Skipped 0\n",
      "Class 4: Copied 200, Skipped 0\n",
      "Class 5: Copied 200, Skipped 0\n",
      "\n",
      "Total images: 1200, Images copied per class: 0:200, 1:200, 2:200, 3:200, 4:200, 5:200\n",
      "\n",
      "Number of images after splitting:\n",
      "\n",
      "Training images: 960, Testing images: 120, Validation images: 120\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "path_dir = './'\n",
    "json_file = './dataset/data.json'\n",
    "img_size = 28\n",
    "max_images_per_class=200\n",
    "\n",
    "# Load and split data\n",
    "train_data, test_data, val_data = load_and_process_data(path_dir, json_file, img_size, max_images_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "X5m-zvJU-uLt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(960, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# print(train_data)\n",
    "# print(train_data[0])\n",
    "# print(train_data[1])\n",
    "print(train_data[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dLg6skbIZu3"
   },
   "source": [
    "#### **Step 2:** Set up the quantum device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "i3TjbZT9tj9U"
   },
   "outputs": [],
   "source": [
    "# Set the number of qubits\n",
    "num_qubits = 10\n",
    "\n",
    "# Set up the quantum devise\n",
    "dev = qml.device('default.qubit', wires=num_qubits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XY3wXMa2Itq9"
   },
   "source": [
    "#### **Step 3:** Define the quantum circuit that encodes features into quantum states\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "m0K__0dHIrrK"
   },
   "outputs": [],
   "source": [
    "# Define the quantum circuit\n",
    "@qml.qnode(dev, interface='torch')\n",
    "def quantum_circuit(features, weights):\n",
    "    # Embed the input features into the quantum state using amplitude encoding\n",
    "    qml.templates.AmplitudeEmbedding(features, wires=range(num_qubits), pad_with=0.0, normalize=True)\n",
    "    \n",
    "    # Apply entangling layers with the specified weights\n",
    "    qml.templates.StronglyEntanglingLayers(weights, wires=range(num_qubits))\n",
    "\n",
    "    # Return the expected values of the Pauli Z operator for each qubit\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(num_qubits)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "wCRJtzwHTVN6"
   },
   "outputs": [],
   "source": [
    "# Function to compute the cost\n",
    "def cost(weights, features, labels):\n",
    "    # Calculate logits as a single tensor per sample\n",
    "    logits_list = [torch.stack(quantum_circuit(features[i], weights)) for i in range(len(features))]\n",
    "    \n",
    "    # print(f\"Logits list: {logits_list}\")\n",
    "    logits = torch.stack(logits_list, dim=0)\n",
    "    loss = F.cross_entropy(logits, labels)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rHRLg2EOZk_"
   },
   "source": [
    "#### **Step 4:** Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "P6rPEei_Tbh9"
   },
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "train_imgs = torch.from_numpy(train_data[0]).to(torch.float32)\n",
    "train_labels = torch.from_numpy(train_data[1]).to(torch.long)\n",
    "\n",
    "# Ensure the data is correctly shaped for our quantum model\n",
    "train_imgs = train_imgs.view(train_imgs.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "LIE6zuIXS3Cw"
   },
   "outputs": [],
   "source": [
    "# print(train_imgs)\n",
    "# print(train_imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1-hQP9eKTxvc",
    "outputId": "b2e61d6f-582e-4c70-a221-1afa80719e30"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pacome/anaconda3/envs/Pennylane/lib/python3.11/site-packages/torch/autograd/graph.py:768: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 2.284844317950581\n",
      "Epoch 1: Loss = 2.263743296623548\n",
      "Epoch 2: Loss = 2.214215088185398\n",
      "Epoch 3: Loss = 2.188797532084825\n",
      "Epoch 4: Loss = 2.173061530704898\n",
      "Epoch 5: Loss = 2.157294200696742\n",
      "Epoch 6: Loss = 2.1553725716332788\n",
      "Epoch 7: Loss = 2.1551297302211365\n",
      "Epoch 8: Loss = 2.1475877851036165\n",
      "Epoch 9: Loss = 2.1370265210204944\n",
      "Epoch 10: Loss = 2.127321971698859\n",
      "Epoch 11: Loss = 2.121958777335858\n",
      "Epoch 12: Loss = 2.1093852474902595\n",
      "Epoch 13: Loss = 2.10261554544224\n",
      "Epoch 14: Loss = 2.0886717832478174\n",
      "Epoch 15: Loss = 2.073536841467328\n",
      "Epoch 16: Loss = 2.0706960050835215\n",
      "Epoch 17: Loss = 2.072096633014854\n",
      "Epoch 18: Loss = 2.0721089622232043\n",
      "Epoch 19: Loss = 2.069074466149156\n",
      "Epoch 20: Loss = 2.0665265032644196\n",
      "Epoch 21: Loss = 2.0649103907457116\n",
      "Epoch 22: Loss = 2.0617716777497144\n",
      "Epoch 23: Loss = 2.0601228287112496\n",
      "Epoch 24: Loss = 2.0563171580636452\n",
      "Epoch 25: Loss = 2.054847179907072\n",
      "Epoch 26: Loss = 2.053631453775288\n",
      "Epoch 27: Loss = 2.0512473531627213\n",
      "Epoch 28: Loss = 2.048781503814094\n",
      "Epoch 29: Loss = 2.0475308633372733\n",
      "Epoch 30: Loss = 2.04578396528768\n",
      "Epoch 31: Loss = 2.044091419061906\n",
      "Epoch 32: Loss = 2.0423905369098176\n",
      "Epoch 33: Loss = 2.039845237715435\n",
      "Epoch 34: Loss = 2.037837483282873\n",
      "Epoch 35: Loss = 2.0359873783482696\n",
      "Epoch 36: Loss = 2.034821881109583\n",
      "Epoch 37: Loss = 2.033983384410147\n",
      "Epoch 38: Loss = 2.033198305579918\n",
      "Epoch 39: Loss = 2.031787866728919\n",
      "Epoch 40: Loss = 2.0311937195273875\n",
      "Epoch 41: Loss = 2.0318943957682043\n",
      "Epoch 42: Loss = 2.0317462978805323\n",
      "Epoch 43: Loss = 2.031496792910655\n",
      "Epoch 44: Loss = 2.030628275682984\n",
      "Epoch 45: Loss = 2.0301939570633403\n",
      "Epoch 46: Loss = 2.0298693237963055\n",
      "Epoch 47: Loss = 2.029330004831864\n",
      "Epoch 48: Loss = 2.0293567482980164\n",
      "Epoch 49: Loss = 2.0293448177146884\n"
     ]
    }
   ],
   "source": [
    "# Number of layers\n",
    "num_layers = 3\n",
    "\n",
    "# Define the shape of the weights: (layers, qubits, parameters per rotation)\n",
    "shape = (num_layers, num_qubits, 3)\n",
    "\n",
    "# Initialize the weights\n",
    "weights = torch.tensor(pnp.random.random(shape), dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# Define the optimizer\n",
    "opt = torch.optim.Adam([weights], lr=0.3)\n",
    "\n",
    "# Define the number of epochs\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Use the train_imgs and train_labels\n",
    "    batch_features, batch_labels = train_imgs, train_labels\n",
    "\n",
    "    # Flatten the features\n",
    "    batch_features = batch_features.view(batch_features.size(0), -1)\n",
    "    # print(f\"Batch features shape: {batch_features.shape}\")\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss = cost(weights, batch_features, batch_labels)\n",
    "    # print(f\"Loss: {loss}\")\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    # Print the loss\n",
    "    print(f\"Epoch {epoch}: Loss = {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluation function\n",
    "def evaluate_model(weights, features, labels):\n",
    "    with torch.no_grad():\n",
    "        logits_list = [torch.stack(quantum_circuit(features[i], weights)) for i in range(len(features))]\n",
    "        logits = torch.stack(logits_list, dim=0)\n",
    "        loss = F.cross_entropy(logits, labels).item()\n",
    "        \n",
    "        # Get predicted classes\n",
    "        predicted_classes = logits.argmax(dim=1).cpu().numpy()\n",
    "        true_classes = labels.cpu().numpy()\n",
    "        \n",
    "        accuracy = accuracy_score(true_classes, predicted_classes)\n",
    "        \n",
    "        return loss, accuracy, predicted_classes, true_classes, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert test and validation data to PyTorch tensors\n",
    "test_imgs = torch.from_numpy(test_data[0]).to(torch.float32)\n",
    "test_labels = torch.from_numpy(test_data[1]).to(torch.long)\n",
    "val_imgs = torch.from_numpy(val_data[0]).to(torch.float32)\n",
    "val_labels = torch.from_numpy(val_data[1]).to(torch.long)\n",
    "\n",
    "# Ensure the data is correctly shaped\n",
    "test_imgs = test_imgs.view(test_imgs.size(0), -1)\n",
    "val_imgs = val_imgs.view(val_imgs.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test and validation sets\n",
    "test_loss, test_accuracy, test_pred_classes, test_true_classes, test_logits = evaluate_model(weights, test_imgs, test_labels)\n",
    "val_loss, val_accuracy, val_pred_classes, val_true_classes, val_logits = evaluate_model(weights, val_imgs, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "test_accuracy = accuracy_score(test_labels.numpy(), test_preds.numpy())\n",
    "val_accuracy = accuracy_score(val_labels.numpy(), val_preds.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Pennylane",
   "language": "python",
   "name": "pennylane"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
